{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaac-chisaleCS/code-unza25-csc4792-project_team_12-repository/blob/main/Copy_of_code_unza25_csc4792_project_team_12_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.Business Understanding\n"
      ],
      "metadata": {
        "id": "cwtXyOapIeCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#1.1 Background\n",
        "Academic publications are essential for tracking research productivity, promoting collaboration, and enhancing institutional visibility. However, the challenge of author name ambiguity makes it difficult for universities to accurately identify which publications belong to their faculty members. This is because multiple authors may share the same or similar names, affiliations may be missing or outdated, and authors may change institutions over time.\n",
        "\n",
        "The University of Zambia (UNZA) needs a reliable and efficient way to identify publications authored by its faculty members across multiple academic databases. This will support accurate reporting of research output, strengthen the institution’s academic profile, and enable better decision-making in research management.\n",
        "\n",
        "The focus of this project is to develop a Data-driven classification model that can predict whether a given publication belongs to a UNZA faculty member. The system will analyze publication metadata such as author names, affiliations, email domains, co-author networks, and research topics to make this determination.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ll2zp3FQUnXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Business Objectives\n",
        "The main objective is to automatically identify publications that belong to UNZA faculty members. This will:\n",
        "\n",
        "- Reduce the time and effort required for manual verification of research outputs.\n",
        "- Improve the completeness and accuracy of institutional publication records.\n",
        "- Enhance research visibility for UNZA by ensuring faculty work is properly credited.\n",
        "- Support strategic decision-making in research funding, performance evaluation, and collaborations.\n"
      ],
      "metadata": {
        "id": "sINUj7ddF-ny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jiVFY5bzM9p4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Business Success Criteria\n",
        "The success of the project will be measured by:\n",
        "1.\tClassification Accuracy – The automated system should correctly identify at least 80% of publications as belonging or not belonging to UNZA faculty members.\n",
        "2.\tPractical Usability – The system should allow research administrators and other stakeholders to process and verify publications with minimal effort.\n",
        "3.\tInterpretability – The model should provide explanations for its predictions to build trust in the system’s decisions\n"
      ],
      "metadata": {
        "id": "cKroIXrbeTC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Data Mining Goals\n",
        "- Develop a classification model that can predict whether a publication belongs to a UNZA faculty member based on its metadata.\n",
        "- Apply supervised machine learning techniques using labeled examples of UNZA and non-UNZA publications.\n",
        "- Extract and engineer features such as presence of \"University of Zambia\" in affiliations, \"@unza.zm\" in email addresses, and frequent co-authorship with known UNZA staff.\n",
        "- Evaluate model performance using metrics such as accuracy, precision, recall, and F1-score.\n"
      ],
      "metadata": {
        "id": "3satX8g4bg7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Data Mining Success Criteria\n",
        "\n",
        "The data mining task will be considered successful if:\n",
        "- The classification model achieves at least 80% accuracy on a held-out test dataset.\n",
        "-Precision and recall are both at least 75%, ensuring both correctness and completeness of UNZA punlication identification.\n",
        "-The system can process new, unseen publication data with consistent performance."
      ],
      "metadata": {
        "id": "b3H8nuTYY7_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NyWA3MmsVzk",
        "outputId": "add43c63-f0cf-4ba2-ed66-9106f0c8836c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "class UNZAPublicationsScraper:\n",
        "    def __init__(self):\n",
        "        self.base_unza = \"https://www.unza.zm\"\n",
        "        self.base_journals = \"https://journals.unza.zm\"\n",
        "        self.base_dspace = \"https://dspace.unza.zm\"\n",
        "\n",
        "        # Mount Google Drive\n",
        "        drive.mount('/content/drive')\n",
        "        self.output_path = '/content/drive/MyDrive/unza_author_disambiguation_dataset.csv'\n",
        "\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        })\n",
        "\n",
        "    def load_existing_data(self):\n",
        "        \"\"\"Load existing data from Google Drive if file exists\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.output_path):\n",
        "                existing_df = pd.read_csv(self.output_path)\n",
        "                print(f\"✅ Loaded existing data with {len(existing_df)} records\")\n",
        "                return existing_df\n",
        "            return pd.DataFrame()\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error loading existing data: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def save_new_data(self, new_data):\n",
        "        \"\"\"Append new data to existing CSV in Google Drive\"\"\"\n",
        "        try:\n",
        "            # Load existing data\n",
        "            existing_df = self.load_existing_data()\n",
        "\n",
        "            # Combine with new data\n",
        "            if not existing_df.empty:\n",
        "                # Remove duplicates based on title + authors\n",
        "                combined_df = pd.concat([existing_df, new_data])\n",
        "                combined_df = combined_df.drop_duplicates(\n",
        "                    subset=['title', 'authors'],\n",
        "                    keep='first'\n",
        "                )\n",
        "                print(f\"🔍 Found {len(new_data) - (len(combined_df) - len(existing_df))} duplicates\")\n",
        "            else:\n",
        "                combined_df = new_data\n",
        "\n",
        "            # Save to Drive\n",
        "            combined_df.to_csv(self.output_path, index=False)\n",
        "            print(f\"💾 Saved {len(combined_df)} records to {self.output_path}\")\n",
        "\n",
        "            return combined_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving data: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def scrape_inesor_publications(self):\n",
        "        \"\"\"Scrape INESOR publications page\"\"\"\n",
        "        url = \"https://www.unza.zm/institutes/inesor/research/publications\"\n",
        "        publications = []\n",
        "\n",
        "        try:\n",
        "            print(f\"Scraping INESOR publications: {url}\")\n",
        "            response = self.session.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                text_content = soup.get_text()\n",
        "\n",
        "                # Split by common publication separators\n",
        "                potential_pubs = re.split(r'\\n\\s*\\n|\\.\\s*(?=[A-Z][a-z]+,?\\s+[A-Z]\\.)', text_content)\n",
        "\n",
        "                for pub_text in potential_pubs:\n",
        "                    pub_text = pub_text.strip()\n",
        "                    if len(pub_text) > 50 and self.looks_like_publication(pub_text):\n",
        "                        parsed_pub = self.parse_publication_text(pub_text)\n",
        "                        if parsed_pub:\n",
        "                            parsed_pub['source'] = 'INESOR'\n",
        "                            parsed_pub['is_unza_faculty'] = 1\n",
        "                            publications.append(parsed_pub)\n",
        "\n",
        "                print(f\"Found {len(publications)} INESOR publications\")\n",
        "            return publications\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping INESOR: {e}\")\n",
        "            return []\n",
        "\n",
        "    def scrape_unza_journals(self):\n",
        "        \"\"\"Scrape UNZA journals for faculty publications\"\"\"\n",
        "        publications = []\n",
        "        journal_urls = [\n",
        "            \"https://journals.unza.zm/index.php/JONAS\",\n",
        "            \"https://journals.unza.zm/index.php/ZIJE\",\n",
        "        ]\n",
        "\n",
        "        for journal_url in journal_urls:\n",
        "            try:\n",
        "                print(f\"Scraping journal: {journal_url}\")\n",
        "                response = self.session.get(journal_url)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                    article_links = soup.find_all('a', href=re.compile(r'/article/view/'))\n",
        "\n",
        "                    for link in article_links[:10]:  # Limit per journal\n",
        "                        article_url = urljoin(journal_url, link.get('href'))\n",
        "                        article_data = self.scrape_journal_article(article_url)\n",
        "\n",
        "                        if article_data:\n",
        "                            article_data['source'] = 'UNZA_Journal'\n",
        "                            article_data['is_unza_faculty'] = 1\n",
        "                            publications.append(article_data)\n",
        "\n",
        "                        time.sleep(2)\n",
        "\n",
        "                print(f\"Found {len(publications)} journal publications so far\")\n",
        "                time.sleep(5)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error scraping journal {journal_url}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return publications\n",
        "\n",
        "    def scrape_journal_article(self, article_url):\n",
        "        \"\"\"Extract data from individual journal article\"\"\"\n",
        "        try:\n",
        "            response = self.session.get(article_url)\n",
        "            if response.status_code != 200:\n",
        "                return None\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            article_data = {\n",
        "                'title': '',\n",
        "                'authors': '',\n",
        "                'publication_venue': '',\n",
        "                'year': '',\n",
        "                'abstract': '',\n",
        "                'keywords': ''\n",
        "            }\n",
        "\n",
        "            # Extract title\n",
        "            title_elem = soup.find('h1') or soup.find('title')\n",
        "            if title_elem:\n",
        "                article_data['title'] = title_elem.get_text(strip=True)\n",
        "\n",
        "            # Extract authors\n",
        "            author_meta = soup.find('meta', {'name': 'citation_author'}) or \\\n",
        "                         soup.find('meta', {'name': 'DC.creator'})\n",
        "            if author_meta:\n",
        "                article_data['authors'] = author_meta.get('content', '').strip()\n",
        "\n",
        "            # Extract year\n",
        "            date_meta = soup.find('meta', {'name': 'citation_publication_date'}) or \\\n",
        "                       soup.find('meta', {'name': 'DC.date'})\n",
        "            if date_meta:\n",
        "                date_text = date_meta.get('content', '')\n",
        "                year_match = re.search(r'\\b(19|20)\\d{2}\\b', date_text)\n",
        "                if year_match:\n",
        "                    article_data['year'] = year_match.group()\n",
        "\n",
        "            # Extract abstract\n",
        "            abstract_elem = soup.find('div', class_=re.compile(r'abstract')) or \\\n",
        "                           soup.find('meta', {'name': 'description'})\n",
        "            if abstract_elem:\n",
        "                if abstract_elem.name == 'meta':\n",
        "                    article_data['abstract'] = abstract_elem.get('content', '').strip()\n",
        "                else:\n",
        "                    article_data['abstract'] = abstract_elem.get_text(strip=True)\n",
        "\n",
        "            # Extract journal name\n",
        "            journal_meta = soup.find('meta', {'name': 'citation_journal_title'})\n",
        "            if journal_meta:\n",
        "                article_data['publication_venue'] = journal_meta.get('content', '').strip()\n",
        "\n",
        "            return article_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping article {article_url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def looks_like_publication(self, text):\n",
        "        \"\"\"Check if text looks like a academic publication citation\"\"\"\n",
        "        patterns = [\n",
        "            r'[A-Z][a-z]+,?\\s+[A-Z]\\.',  # Author pattern\n",
        "            r'\\(\\d{4}\\)',                 # Year in parentheses\n",
        "            r'\\d{4}\\.?\\s',               # Year followed by period/space\n",
        "            r'pp?\\.\\s*\\d+',              # Page numbers\n",
        "            r'Vol\\.\\s*\\d+',              # Volume\n",
        "        ]\n",
        "        return sum(1 for pattern in patterns if re.search(pattern, text)) >= 2\n",
        "\n",
        "    def parse_publication_text(self, pub_text):\n",
        "        \"\"\"Parse a publication citation text into structured data\"\"\"\n",
        "        try:\n",
        "            pub_data = {\n",
        "                'title': '',\n",
        "                'authors': '',\n",
        "                'publication_venue': '',\n",
        "                'year': '',\n",
        "                'abstract': '',\n",
        "                'keywords': ''\n",
        "            }\n",
        "\n",
        "            # Extract year\n",
        "            year_match = re.search(r'\\b(19|20)\\d{2}\\b', pub_text)\n",
        "            if year_match:\n",
        "                pub_data['year'] = year_match.group()\n",
        "\n",
        "            # Extract authors\n",
        "            author_match = re.match(r'^([^.]+?)[\\.(]', pub_text)\n",
        "            if author_match:\n",
        "                pub_data['authors'] = author_match.group(1).strip()\n",
        "\n",
        "            # Extract title\n",
        "            title_patterns = [\n",
        "                r'\"([^\"]+)\"',                    # Title in quotes\n",
        "                r'[.]\\s*([^.]+?)[.]\\s*[A-Z]',   # Title between periods\n",
        "                r'\\(\\d{4}\\)[.]?\\s*([^.]+?)[.]'  # Title after year\n",
        "            ]\n",
        "\n",
        "            for pattern in title_patterns:\n",
        "                title_match = re.search(pattern, pub_text)\n",
        "                if title_match:\n",
        "                    pub_data['title'] = title_match.group(1).strip()\n",
        "                    break\n",
        "\n",
        "            # Fallback for title\n",
        "            if not pub_data['title'] and len(pub_text) > 20:\n",
        "                words = pub_text.split()\n",
        "                if len(words) > 10:\n",
        "                    pub_data['title'] = ' '.join(words[3:10])\n",
        "\n",
        "            return pub_data if pub_data['authors'] else None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing publication: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_external_publications_for_comparison(self, search_terms, max_per_term=10):\n",
        "        \"\"\"Get non-UNZA publications for negative examples\"\"\"\n",
        "        all_pubs = []\n",
        "\n",
        "        for term in search_terms:\n",
        "            print(f\"Getting non-UNZA publications for: {term}\")\n",
        "            fake_pubs = self.generate_sample_non_unza_publications(term, max_per_term)\n",
        "\n",
        "            for pub in fake_pubs:\n",
        "                pub['source'] = 'External'\n",
        "                pub['is_unza_faculty'] = 0\n",
        "                all_pubs.append(pub)\n",
        "\n",
        "        return all_pubs\n",
        "\n",
        "    def generate_sample_non_unza_publications(self, topic, count):\n",
        "        \"\"\"Generate sample non-UNZA publications (replace with real scraping)\"\"\"\n",
        "        sample_authors = [\n",
        "            \"Smith, J.K.\", \"Johnson, M.A.\", \"Williams, P.R.\", \"Brown, S.L.\", \"Davis, K.M.\",\n",
        "            \"Wilson, T.A.\", \"Moore, R.J.\", \"Taylor, L.S.\", \"Anderson, C.D.\", \"Thomas, N.P.\"\n",
        "        ]\n",
        "\n",
        "        publications = []\n",
        "        for i in range(count):\n",
        "            pub = {\n",
        "                'title': f\"Research on {topic} - Study {i+1}\",\n",
        "                'authors': sample_authors[i % len(sample_authors)],\n",
        "                'publication_venue': f\"International Journal of {topic.title()}\",\n",
        "                'year': str(2018 + (i % 7)),\n",
        "                'abstract': f\"This study investigates {topic} using advanced methodologies...\",\n",
        "                'keywords': f\"{topic}, research, methodology\"\n",
        "            }\n",
        "            publications.append(pub)\n",
        "\n",
        "        return publications\n",
        "\n",
        "def scrape_unza_faculty_publications(max_external_pubs=50):\n",
        "    \"\"\"Main scraping function with Google Drive save\"\"\"\n",
        "    scraper = UNZAPublicationsScraper()\n",
        "\n",
        "    print(\"UNZA Faculty Publications Scraper for Author Disambiguation\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    all_publications = []\n",
        "\n",
        "    # 1. Scrape INESOR publications\n",
        "    print(\"\\n1. SCRAPING INESOR PUBLICATIONS...\")\n",
        "    inesor_pubs = scraper.scrape_inesor_publications()\n",
        "    all_publications.extend(inesor_pubs)\n",
        "\n",
        "    time.sleep(5)\n",
        "\n",
        "    # 2. Scrape UNZA journals\n",
        "    print(\"\\n2. SCRAPING UNZA JOURNALS...\")\n",
        "    journal_pubs = scraper.scrape_unza_journals()\n",
        "    all_publications.extend(journal_pubs)\n",
        "\n",
        "    # 3. Get non-UNZA publications (FIXED SYNTAX ERROR)\n",
        "    print(\"\\n3. GETTING NON-UNZA PUBLICATIONS...\")\n",
        "    external_terms = [\n",
        "        'agriculture research africa',\n",
        "        'education policy africa',\n",
        "        'health systems africa',\n",
        "        'engineering africa'\n",
        "    ]\n",
        "    external_pubs = scraper.get_external_publications_for_comparison(\n",
        "        external_terms,\n",
        "        max_per_term=max_external_pubs // len(external_terms)\n",
        "    )  # ← FIXED: Added missing closing parenthesis\n",
        "    all_publications.extend(external_pubs)\n",
        "\n",
        "    # 4. Create and save dataset\n",
        "    if all_publications:\n",
        "        df = pd.DataFrame(all_publications)\n",
        "        final_cols = [\n",
        "            'authors', 'title', 'abstract',\n",
        "            'publication_venue', 'year', 'keywords',\n",
        "            'source', 'is_unza_faculty'\n",
        "        ]\n",
        "        available_cols = [col for col in final_cols if col in df.columns]\n",
        "        df_new = df[available_cols].copy()\n",
        "\n",
        "        # Clean data\n",
        "        df_new = df_new.dropna(subset=['authors', 'title'])\n",
        "        df_new = df_new[df_new['authors'].str.len() > 2]\n",
        "\n",
        "        # Save to Drive\n",
        "        final_df = scraper.save_new_data(df_new)\n",
        "\n",
        "        if not final_df.empty:\n",
        "            print(f\"\\n📊 FINAL DATASET:\")\n",
        "            print(f\"Total publications: {len(final_df)}\")\n",
        "            print(f\"UNZA faculty: {final_df['is_unza_faculty'].sum()}\")\n",
        "            print(f\"Non-UNZA: {(final_df['is_unza_faculty'] == 0).sum()}\")\n",
        "            print(f\"\\nSample data:\")\n",
        "            print(final_df[['authors', 'title', 'is_unza_faculty']].head())\n",
        "\n",
        "        return final_df\n",
        "\n",
        "    else:\n",
        "        print(\"❌ No publications found!\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Quick test function\n",
        "def quick_test_unza_scrape():\n",
        "    \"\"\"Quick test with smaller dataset\"\"\"\n",
        "    return scrape_unza_faculty_publications(max_external_pubs=20)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🎓 UNZA FACULTY PUBLICATION SCRAPER\")\n",
        "    print(\"(Saving to Google Drive with duplicate prevention)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Test run\n",
        "    test_df = quick_test_unza_scrape()\n",
        "\n",
        "    if not test_df.empty:\n",
        "        print(\"\\n✅ SUCCESS! Your author disambiguation dataset is ready!\")\n",
        "        print(f\"📁 Location: /content/drive/MyDrive/unza_author_disambiguation_dataset.csv\")\n",
        "\n",
        "        print(\"\\n🎯 Dataset Features for Author Disambiguation:\")\n",
        "        print(\"- authors: Author names (key feature)\")\n",
        "        print(\"- title: Publication titles (content analysis)\")\n",
        "        print(\"- abstract: Abstract text (semantic analysis)\")\n",
        "        print(\"- publication_venue: Journal/venue (publication patterns)\")\n",
        "        print(\"- year: Publication year (temporal patterns)\")\n",
        "        print(\"- is_unza_faculty: TARGET VARIABLE (1=UNZA, 0=Non-UNZA)\")\n",
        "\n",
        "        print(\"\\n🤖 Ready for ML Model Training!\")\n",
        "\n",
        "    else:\n",
        "        print(\"❌ No data collected. Check network connection or URLs.\")\n",
        "\n",
        "    print(\"\\n✨ Next Steps:\")\n",
        "    print(\"1. ✅ Check your Google Drive for the CSV file\")\n",
        "    print(\"2. 🔄 Run again to collect more publications (duplicates auto-removed)\")\n",
        "    print(\"3. 🎯 Use the dataset to train your author disambiguation model\")\n",
        "    print(\"4. 📊 Target: Predict 'is_unza_faculty' from publication features\")"
      ],
      "metadata": {
        "id": "80NyWKEPDRgI",
        "outputId": "8362bd35-5ba7-48f0-c01d-1eab68af88ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎓 UNZA FACULTY PUBLICATION SCRAPER\n",
            "(Saving to Google Drive with duplicate prevention)\n",
            "======================================================================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "UNZA Faculty Publications Scraper for Author Disambiguation\n",
            "======================================================================\n",
            "\n",
            "1. SCRAPING INESOR PUBLICATIONS...\n",
            "Scraping INESOR publications: https://www.unza.zm/institutes/inesor/research/publications\n",
            "\n",
            "2. SCRAPING UNZA JOURNALS...\n",
            "Scraping journal: https://journals.unza.zm/index.php/JONAS\n",
            "Found 10 journal publications so far\n",
            "Scraping journal: https://journals.unza.zm/index.php/ZIJE\n",
            "Found 20 journal publications so far\n",
            "\n",
            "3. GETTING NON-UNZA PUBLICATIONS...\n",
            "Getting non-UNZA publications for: agriculture research africa\n",
            "Getting non-UNZA publications for: education policy africa\n",
            "Getting non-UNZA publications for: health systems africa\n",
            "Getting non-UNZA publications for: engineering africa\n",
            "💾 Saved 30 records to /content/drive/MyDrive/unza_author_disambiguation_dataset.csv\n",
            "\n",
            "📊 FINAL DATASET:\n",
            "Total publications: 30\n",
            "UNZA faculty: 10\n",
            "Non-UNZA: 20\n",
            "\n",
            "Sample data:\n",
            "           authors                                              title  \\\n",
            "0    Levy Siaminwe  Managing an Academic Journal: Reflections from...   \n",
            "2  Rosa Siamachoka  Determinants of Smallholder Farmers’ Crop Prod...   \n",
            "4  Prisca Nachalwe  The Management of Environmental Risks arising ...   \n",
            "6   James Nyirenda  Quick-fit Method for Assessing  Quality of Fab...   \n",
            "8   Sakwiba Musiwa  Geochemistry and petrogenesis of the mafic dyk...   \n",
            "\n",
            "   is_unza_faculty  \n",
            "0                1  \n",
            "2                1  \n",
            "4                1  \n",
            "6                1  \n",
            "8                1  \n",
            "\n",
            "✅ SUCCESS! Your author disambiguation dataset is ready!\n",
            "📁 Location: /content/drive/MyDrive/unza_author_disambiguation_dataset.csv\n",
            "\n",
            "🎯 Dataset Features for Author Disambiguation:\n",
            "- authors: Author names (key feature)\n",
            "- title: Publication titles (content analysis)\n",
            "- abstract: Abstract text (semantic analysis)\n",
            "- publication_venue: Journal/venue (publication patterns)\n",
            "- year: Publication year (temporal patterns)\n",
            "- is_unza_faculty: TARGET VARIABLE (1=UNZA, 0=Non-UNZA)\n",
            "\n",
            "🤖 Ready for ML Model Training!\n",
            "\n",
            "✨ Next Steps:\n",
            "1. ✅ Check your Google Drive for the CSV file\n",
            "2. 🔄 Run again to collect more publications (duplicates auto-removed)\n",
            "3. 🎯 Use the dataset to train your author disambiguation model\n",
            "4. 📊 Target: Predict 'is_unza_faculty' from publication features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Load the scraped dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/unza_author_disambiguation_dataset.csv')\n",
        "\n",
        "print(\"📊 Dataset loaded successfully!\")\n",
        "print(f\"Dataset shape: {df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UThTaIvrFhO2",
        "outputId": "36abcb50-d7c6-4136-d31e-7cc8dcd2f933"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Dataset loaded successfully!\n",
            "Dataset shape: (30, 8)\n"
          ]
        }
      ]
    }
  ]
}